{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the comment : \n",
      "mahinda hora\n",
      "positive probability of the comments is : 0.00898527564653457\n",
      "negative probability of the comments is : 0.0026746506274597163\n",
      "This comment is a positive comment and perplexity is : 3.2480084379056002\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "def preprocess_comment(comments):\n",
    "\n",
    "    remove_punc = str.maketrans('', '', string.punctuation)\n",
    "    Comments_without_punc = comments.translate(remove_punc)\n",
    "    \n",
    "    comments_without_whitespaces = \" \".join(Comments_without_punc.split())\n",
    "   \n",
    "    special_char = r'[^a-zA-Z0-9\\s]' \n",
    "    comments_without_special_char = re.sub(special_char, '', comments_without_whitespaces)\n",
    "   \n",
    "    comments_final = \"< \" + comments_without_special_char + \" >\"\n",
    "    \n",
    "    tokens = word_tokenize(comments_final.lower())\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def smoothing(t,s,v):\n",
    "    \n",
    "    count = (t+1) / (s+v)\n",
    "    return count\n",
    "    \n",
    "def perplexity(probability,word_count):\n",
    "    #find\n",
    "    perp = pow(probability, (-1 / word_count))\n",
    "    return perp\n",
    "\n",
    "def probability(uni_Freq, bigram_Freq, comment):\n",
    "    #print(comment)\n",
    "    probability = 1.0\n",
    "    multy_prob = 0.0\n",
    "    for i in range(len(comment)):\n",
    "        \n",
    "        if comment[i] in bigram_Freq:\n",
    "            #print(comment[i])\n",
    "            t = bigram_Freq[comment[i]]\n",
    "            #print(t)\n",
    "        else:\n",
    "            t = 0\n",
    "        if comment[i][0] in uni_Freq:\n",
    "            #print(comment[i][0])\n",
    "            s = uni_Freq[comment[i][0]]\n",
    "            #print(s)\n",
    "        else:\n",
    "            s = 0\n",
    "        #print(t)\n",
    "        #print(s)\n",
    "        probability = probability * smoothing(t,s,len(uni_Freq))\n",
    "        multy_prob = multy_prob + probability\n",
    "    return multy_prob\n",
    "    \n",
    "#train data sets\n",
    "def train_comments(comment,a):\n",
    "    \n",
    "    uni_list = []\n",
    "    bigram_list = []\n",
    "   \n",
    "    #preprocessing\n",
    "    for line in a:\n",
    "        \n",
    "        tokens = preprocess_comment(line)\n",
    "        #unigrams\n",
    "        uni_list = uni_list + list(tokens)\n",
    "        #bigrams  \n",
    "        bigram_comments = nltk.bigrams(tokens)\n",
    "        bigram_list = bigram_list + list(bigram_comments)\n",
    "    \n",
    "    uni_Freq = FreqDist(uni_list)\n",
    "    bigram_Freq = FreqDist(bigram_list)\n",
    "    probability_of_comment = probability(uni_Freq, bigram_Freq, comment)\n",
    "    \n",
    "    return probability_of_comment\n",
    "        \n",
    "\n",
    "    \n",
    "#open negative corpus\n",
    "with open ('negative.txt', 'rt',encoding='utf-8') as text_file:  \n",
    "    extracted_negative_text = text_file.readlines() \n",
    "#text_file.close()\n",
    "\n",
    "#open positive corpus\n",
    "with open ('positive.txt', 'rt',encoding='utf-8') as text_file:  \n",
    "    extracted_positive_text = text_file.readlines() \n",
    "#text_file.close()\n",
    "        \n",
    "#testing\n",
    "print(\"Enter the comment : \")\n",
    "new_comment = input()\n",
    "new_comment_tokens = preprocess_comment(new_comment)\n",
    "new_comment_bigrams = list(nltk.bigrams(new_comment_tokens))\n",
    "word_count = len(new_comment_tokens)\n",
    "\n",
    "positive_prob = train_comments(new_comment_bigrams, extracted_positive_text)\n",
    "negative_prob = train_comments(new_comment_bigrams, extracted_negative_text)\n",
    "\n",
    "print(\"positive probability of the comments is : \"+str(positive_prob))\n",
    "print(\"negative probability of the comments is : \"+str(negative_prob))\n",
    "\n",
    "if positive_prob > negative_prob:\n",
    "    \n",
    "    perp = perplexity(positive_prob,word_count)\n",
    "    print(\"This comment is a positive comment and perplexity is : \" +str(perp))\n",
    "\n",
    "else:\n",
    "    if negative_prob > positive_prob :\n",
    "        perp = perplexity(negative_prob,word_count)\n",
    "        print(\"This comment is a negative comment and perplexity is : \" +str(perp))\n",
    "    \n",
    "    else :\n",
    "        print(\"This comment is neutral\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
